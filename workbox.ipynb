{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'h.e.l.l.o'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'.'.join('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t1\tfoo\tbaz\t0\n",
      "1\t0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import hfst\n",
    "tr1 = hfst.regex('foo:bar')\n",
    "tr2 = hfst.regex('bar:baz')\n",
    "tr1.compose(tr2)\n",
    "print(tr1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== a ===\n",
      "=== b ===\n",
      "0\t1\tA\tA\t0\n",
      "1\t2\tb\tb\t0\n",
      "2\t3\ti\ti\t0\n",
      "3\t4\th\th\t0\n",
      "4\t5\tu\tu\t0\n",
      "5\t6\tt\tt\t0\n",
      "6\t7\t<n>\t@0@\t0\n",
      "6\t8\t<n>\tb\t0\n",
      "6\t9\t<n>\tn\t0\n",
      "6\t10\t<n>\ty\t0\n",
      "6\t11\t<n>\t{e}\t0\n",
      "6\t12\t<n>\tm\t0\n",
      "6\t13\t<n>\te\t0\n",
      "6\t14\t<n>\tf\t0\n",
      "6\t15\t<n>\t{a}\t0\n",
      "6\t16\t<n>\t{W}\t0\n",
      "6\t17\t<n>\ts\t0\n",
      "7\t0\n",
      "8\t18\t<relational>\ta\t0\n",
      "9\t19\t<subj>\ta\t0\n",
      "10\t20\t+\ta\t0\n",
      "11\t19\t<ins>\tm\t0\n",
      "12\t21\t<pos>\ta\t0\n",
      "13\t19\t<pos>\tm\t0\n",
      "14\t7\t<dat>\t@0@\t0\n",
      "14\t22\t<com>\ta\t0\n",
      "15\t23\t+\tp\t0\n",
      "16\t24\t+\te\t0\n",
      "17\t7\t<acc>\t@0@\t0\n",
      "18\t25\t@0@\tr\t0\n",
      "19\t0\n",
      "20\t26\ty\tn\t0\n",
      "21\t25\t@0@\tn\t0\n",
      "22\t27\t@0@\tr\t0\n",
      "23\t28\ta\t@0@\t0\n",
      "24\t29\tw\tr\t0\n",
      "25\t27\t@0@\ta\t0\n",
      "26\t30\ta\t@0@\t0\n",
      "27\t0\n",
      "28\t31\tp\t@0@\t0\n",
      "29\t32\te\t@0@\t0\n",
      "30\t33\tn\t@0@\t0\n",
      "31\t7\t<post>\t@0@\t0\n",
      "32\t31\tr\t@0@\t0\n",
      "33\t7\t<neg>\t@0@\t0\n",
      "\n",
      "Abihut<n>:Abihut@_EPSILON_SYMBOL_@\t0\n",
      "Abihut<n><relational>@_EPSILON_SYMBOL_@@_EPSILON_SYMBOL_@:Abihutbara\t0\n",
      "Abihut<n><pos>:Abihutem\t0\n",
      "Abihut<n><com>@_EPSILON_SYMBOL_@:Abihutfar\t0\n",
      "Abihut<n><dat>:Abihutf@_EPSILON_SYMBOL_@\t0\n",
      "Abihut<n><pos>@_EPSILON_SYMBOL_@@_EPSILON_SYMBOL_@:Abihutmana\t0\n",
      "Abihut<n><subj>:Abihutna\t0\n",
      "Abihut<n><acc>:Abihuts@_EPSILON_SYMBOL_@\t0\n",
      "Abihut<n>+yan<neg>:Abihutyan@_EPSILON_SYMBOL_@@_EPSILON_SYMBOL_@@_EPSILON_SYMBOL_@\t0\n",
      "Abihut<n>+wer<post>:Abihut{W}er@_EPSILON_SYMBOL_@@_EPSILON_SYMBOL_@@_EPSILON_SYMBOL_@\t0\n",
      "Abihut<n>+ap<post>:Abihut{a}p@_EPSILON_SYMBOL_@@_EPSILON_SYMBOL_@@_EPSILON_SYMBOL_@\t0\n",
      "Abihut<n><ins>:Abihut{e}m\t0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import hfst\n",
    "import sys\n",
    "\n",
    "a = hfst.compile_lexc_file('apertium-bkl.bkl.lexc')\n",
    "a.determinize()\n",
    "b = hfst.regex('[A b i h u t][?*]:[?*]')\n",
    "b.intersect(a)\n",
    "print('=== a ===')\n",
    "#print(a)\n",
    "print('=== b ===')\n",
    "print(b)\n",
    "print(b.extract_paths(max_cycles=16, max_number=200, output='text', random='True'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== a ===\n",
      "0\t1\thello\ta\t0\n",
      "0\t1\thi\ta\t0\n",
      "0\t1\tokay\ta\t0\n",
      "0\t2\tme\tme\t0\n",
      "0\t2\thello\thello\t0\n",
      "1\t1\t@0@\ta\t0\n",
      "1\t0\n",
      "2\t3\tthere\tb\t0\n",
      "3\t3\t@0@\tb\t0\n",
      "3\t0\n",
      "\n",
      "hello:a\t0\n",
      "hello@_EPSILON_SYMBOL_@:aa\t0\n",
      "hello@_EPSILON_SYMBOL_@@_EPSILON_SYMBOL_@:aaa\t0\n",
      "hello@_EPSILON_SYMBOL_@@_EPSILON_SYMBOL_@@_EPSILON_SYMBOL_@:aaaa\t0\n",
      "hello@_EPSILON_SYMBOL_@@_EPSILON_SYMBOL_@@_EPSILON_SYMBOL_@@_EPSILON_SYMBOL_@:aaaaa\t0\n",
      "hello@_EPSILON_SYMBOL_@@_EPSILON_SYMBOL_@@_EPSILON_SYMBOL_@@_EPSILON_SYMBOL_@@_EPSILON_SYMBOL_@:aaaaaa\t0\n",
      "hi:a\t0\n",
      "hi@_EPSILON_SYMBOL_@:aa\t0\n",
      "hi@_EPSILON_SYMBOL_@@_EPSILON_SYMBOL_@:aaa\t0\n",
      "hi@_EPSILON_SYMBOL_@@_EPSILON_SYMBOL_@@_EPSILON_SYMBOL_@:aaaa\t0\n",
      "\n",
      "=== b ===\n",
      "0\t1\thello\ta\t0\n",
      "0\t2\thello\thello\t0\n",
      "1\t3\t@_UNKNOWN_SYMBOL_@\t@0@\t0\n",
      "1\t3\tthere\t@0@\t0\n",
      "1\t3\thello\t@0@\t0\n",
      "1\t3\tokay\t@0@\t0\n",
      "1\t3\tme\t@0@\t0\n",
      "1\t3\thi\t@0@\t0\n",
      "1\t3\tb\t@0@\t0\n",
      "1\t3\ta\t@0@\t0\n",
      "1\t4\t@0@\ta\t0\n",
      "1\t0\n",
      "2\t5\t@_UNKNOWN_SYMBOL_@\tb\t0\n",
      "2\t5\ta\tb\t0\n",
      "2\t5\tb\tb\t0\n",
      "2\t5\thi\tb\t0\n",
      "2\t5\tme\tb\t0\n",
      "2\t5\tokay\tb\t0\n",
      "2\t5\tthere\tb\t0\n",
      "2\t5\thello\tb\t0\n",
      "2\t6\t@0@\tb\t0\n",
      "3\t3\t@_UNKNOWN_SYMBOL_@\t@0@\t0\n",
      "3\t3\thello\t@0@\t0\n",
      "3\t3\ta\t@0@\t0\n",
      "3\t3\tb\t@0@\t0\n",
      "3\t3\thi\t@0@\t0\n",
      "3\t3\tme\t@0@\t0\n",
      "3\t3\tokay\t@0@\t0\n",
      "3\t3\tthere\t@0@\t0\n",
      "3\t7\t@0@\ta\t0\n",
      "3\t0\n",
      "4\t4\t@0@\ta\t0\n",
      "4\t0\n",
      "5\t8\t@_UNKNOWN_SYMBOL_@\t@0@\t0\n",
      "5\t8\tthere\t@0@\t0\n",
      "5\t8\thello\t@0@\t0\n",
      "5\t8\tokay\t@0@\t0\n",
      "5\t8\tme\t@0@\t0\n",
      "5\t8\thi\t@0@\t0\n",
      "5\t8\tb\t@0@\t0\n",
      "5\t8\ta\t@0@\t0\n",
      "5\t9\t@0@\tb\t0\n",
      "5\t0\n",
      "6\t6\t@0@\tb\t0\n",
      "6\t0\n",
      "7\t7\t@0@\ta\t0\n",
      "7\t0\n",
      "8\t8\t@_UNKNOWN_SYMBOL_@\t@0@\t0\n",
      "8\t8\thello\t@0@\t0\n",
      "8\t8\ta\t@0@\t0\n",
      "8\t8\tb\t@0@\t0\n",
      "8\t8\thi\t@0@\t0\n",
      "8\t8\tme\t@0@\t0\n",
      "8\t8\tokay\t@0@\t0\n",
      "8\t8\tthere\t@0@\t0\n",
      "8\t10\t@0@\tb\t0\n",
      "8\t0\n",
      "9\t9\t@0@\tb\t0\n",
      "9\t0\n",
      "10\t10\t@0@\tb\t0\n",
      "10\t0\n",
      "\n",
      "hello:a\t0\n",
      "hello@_UNKNOWN_SYMBOL_@:a@_EPSILON_SYMBOL_@\t0\n",
      "hello@_UNKNOWN_SYMBOL_@@_EPSILON_SYMBOL_@:a@_EPSILON_SYMBOL_@a\t0\n",
      "hello@_UNKNOWN_SYMBOL_@@_EPSILON_SYMBOL_@@_EPSILON_SYMBOL_@:a@_EPSILON_SYMBOL_@aa\t0\n",
      "hello@_UNKNOWN_SYMBOL_@@_UNKNOWN_SYMBOL_@:a@_EPSILON_SYMBOL_@@_EPSILON_SYMBOL_@\t0\n",
      "hello@_UNKNOWN_SYMBOL_@@_UNKNOWN_SYMBOL_@@_EPSILON_SYMBOL_@:a@_EPSILON_SYMBOL_@@_EPSILON_SYMBOL_@a\t0\n",
      "hello@_UNKNOWN_SYMBOL_@@_UNKNOWN_SYMBOL_@@_EPSILON_SYMBOL_@@_EPSILON_SYMBOL_@:a@_EPSILON_SYMBOL_@@_EPSILON_SYMBOL_@aa\t0\n",
      "hello@_UNKNOWN_SYMBOL_@hello:a@_EPSILON_SYMBOL_@@_EPSILON_SYMBOL_@\t0\n",
      "hello@_UNKNOWN_SYMBOL_@hello@_EPSILON_SYMBOL_@:a@_EPSILON_SYMBOL_@@_EPSILON_SYMBOL_@a\t0\n",
      "hello@_UNKNOWN_SYMBOL_@hello@_EPSILON_SYMBOL_@@_EPSILON_SYMBOL_@:a@_EPSILON_SYMBOL_@@_EPSILON_SYMBOL_@aa\t0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import hfst\n",
    "\n",
    "a2 = hfst.regex('[ hello | me ] [ there ] :[b+]')\n",
    "a  = hfst.regex('[ hello | hi | okay ] :[a+]')\n",
    "a.priority_union(a2)\n",
    "a.determinize()\n",
    "\n",
    "\n",
    "a_basic = hfst.HfstBasicTransducer(a)\n",
    "b = hfst.regex('[ hello ][?*]:[?*]')\n",
    "ab = b.compose(a)\n",
    "\n",
    "print('=== a ===')\n",
    "print(a)\n",
    "# for arc in a_basic.transitions(0):\n",
    "#     print(arc)\n",
    "print(a.extract_paths(max_number=10, max_cycles=5, output='text'))\n",
    "print('=== b ===')\n",
    "print(b)\n",
    "print(b.extract_paths(max_number=10, max_cycles=1, output='text'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pel<n>:pel@_EPSILON_SYMBOL_@\t0\n",
      "pel<n><relational>@_EPSILON_SYMBOL_@@_EPSILON_SYMBOL_@:pelbara\t0\n",
      "pel<n><pos>:pelem\t0\n",
      "pel<n><com>@_EPSILON_SYMBOL_@:pelfar\t0\n",
      "pel<n><dat>:pelf@_EPSILON_SYMBOL_@\t0\n",
      "pel<n><pos>@_EPSILON_SYMBOL_@@_EPSILON_SYMBOL_@:pelmana\t0\n",
      "pel<n><subj>:pelna\t0\n",
      "pel<n><acc>:pels@_EPSILON_SYMBOL_@\t0\n",
      "pel<n>+yan<neg>:pelyan@_EPSILON_SYMBOL_@@_EPSILON_SYMBOL_@@_EPSILON_SYMBOL_@\t0\n",
      "pel<n>+wer<post>:pel{W}er@_EPSILON_SYMBOL_@@_EPSILON_SYMBOL_@@_EPSILON_SYMBOL_@\t0\n",
      "pel<n>+ap<post>:pel{a}p@_EPSILON_SYMBOL_@@_EPSILON_SYMBOL_@@_EPSILON_SYMBOL_@\t0\n",
      "pel<n><ins>:pel{e}m\t0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import hfst\n",
    "\n",
    "tr = hfst.compile_lexc_file('apertium-bkl.bkl.lexc')\n",
    "tr.determinize()\n",
    "\n",
    "def get_tags():\n",
    "    return list(filter(lambda s: s.startswith('<') and s.endswith('>'), tr.get_alphabet()))\n",
    "\n",
    "def expand_root(root):\n",
    "    hfst_regex = '[ %s ][[ %s ][ ?* ]]:[?*]' % (' '.join(root), ' | '.join(\n",
    "        map(\n",
    "            lambda s: s.replace('<', '%<').replace('>', '%>'), \n",
    "            get_tags())))\n",
    "    limiting_tr = hfst.regex(hfst_regex)\n",
    "    tr_copy = hfst.HfstTransducer(tr)\n",
    "    tr_copy.intersect(limiting_tr)\n",
    "    return tr_copy.extract_paths(max_cycles=16, max_number=256, output='text')\n",
    "    \n",
    "print(expand_root('pel'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import hfst\n",
    "\n",
    "class Lexc:\n",
    "    multichar_symbols = list()\n",
    "    lexicons = dict()\n",
    "    \n",
    "    def __init__(self, ms, ls):\n",
    "        self.multichar_symbols = ms\n",
    "        self.lexicons = ls\n",
    "\n",
    "def remove_comment_from_line(line):\n",
    "    return re.split('(?<!%)\\\\!', line)[0]\n",
    "\n",
    "def filter_blank_lines(line):\n",
    "    return line.strip() != ''\n",
    "\n",
    "def separate_symbols_lexicon(lines):\n",
    "    sections = []\n",
    "    current = []\n",
    "    for line in lines:\n",
    "        if line == 'Multichar_Symbols':\n",
    "            if len(current) > 0:\n",
    "                sections.append(current)\n",
    "            current = ['Multichar_Symbols']\n",
    "        elif line.startswith('LEXICON'):\n",
    "            if len(current) > 0:\n",
    "                sections.append(current)\n",
    "            current = [ line ]\n",
    "        else:\n",
    "            current.append(line)\n",
    "    if len(current) > 0:\n",
    "        sections.append(current)\n",
    "    return sections\n",
    "\n",
    "def get_multichar_symbols(sections):\n",
    "    for section in sections:\n",
    "        if len(section) > 0 and section[0] == 'Multichar_Symbols':\n",
    "            return section[1:]\n",
    "    return []\n",
    "\n",
    "def get_lexicons(sections):\n",
    "    lexicons = dict()\n",
    "    for section in sections:\n",
    "        if len(section) > 0 and section[0].startswith('LEXICON'):\n",
    "            x = len('LEXICON ')\n",
    "            name = section[0][x:].strip()\n",
    "            lexicons[name] = list(map(lambda x: x.strip(), re.split('(?<!(?<!%)%);', ''.join(section[1:]))))\n",
    "    return lexicons\n",
    "\n",
    "def tokenize_rule(rule):            \n",
    "    return re.split('(?<!(?<!%)%)\\\\s+', rule)\n",
    "\n",
    "def parse_rules(lexicon):\n",
    "    return list(map(lambda x: list(map(lambda y: y.strip(), x)),\n",
    "        filter(\n",
    "            lambda x: x != [''], \n",
    "            map(\n",
    "                tokenize_rule, \n",
    "                lexicon))))\n",
    "\n",
    "def parse_replacement(replacement):\n",
    "    return list(map(lambda x: '' if x =='0' else x, re.split('(?<!(?<!%)%)\\\\:', replacement)))\n",
    "\n",
    "def get_lexicons_rules(lexicons):\n",
    "    lexicons_rules = dict()\n",
    "    for tag in list(lexicons):\n",
    "        lexicons_rules[tag] = parse_rules(lexicons[tag])\n",
    "    return lexicons_rules\n",
    "\n",
    "def identify_roots(lexicons_rules):\n",
    "    def find_first_replacements(lexicon_tag):\n",
    "        if lexicon_tag in lexicons_rules:\n",
    "            replacements = []\n",
    "            lexicon_rules = lexicons_rules[lexicon_tag]\n",
    "            for rule in lexicon_rules:\n",
    "                if len(rule) == 2:\n",
    "                    replacements.append(parse_replacement(rule[0]))\n",
    "                elif len(rule) == 1:\n",
    "                    replacements.extend(find_first_replacements(rule[0]))\n",
    "            return replacements\n",
    "        else:\n",
    "            return []\n",
    "        \n",
    "    return find_first_replacements('Root')\n",
    "            \n",
    "tr = hfst.compile_lexc_file('apertium-bkl.bkl.lexc')\n",
    "tr.determinize()\n",
    "\n",
    "def get_tags():\n",
    "    return list(filter(lambda s: s.startswith('<') and s.endswith('>'), tr.get_alphabet()))\n",
    "\n",
    "def expand_root(tr, root):\n",
    "    hfst_regex = '[ %s ][[ %s ][ ?* ]]:[ ?* ]' % (' '.join(root), ' | '.join(\n",
    "        map(\n",
    "            lambda s: s.replace('<', '%<').replace('>', '%>'), \n",
    "            get_tags())))\n",
    "#     hfst_regex = '[ %s ][ ?* ]:[ ?* ]' % ' '.join(root)\n",
    "#     print(hfst_regex)\n",
    "    limiting_tr = hfst.regex(hfst_regex)\n",
    "    tr_copy = hfst.HfstTransducer(tr)\n",
    "    tr_copy.intersect(limiting_tr)\n",
    "    return tr_copy.extract_paths(max_cycles=16, max_number=256, output='dict')\n",
    "    \n",
    "def read_lexc(lexc_filename):\n",
    "    with open(lexc_filename) as lexc_file:\n",
    "        lexc = lexc_file.read()\n",
    "        lexc_lines = lexc.splitlines()\n",
    "        no_comments = map(remove_comment_from_line, lexc_lines)\n",
    "        no_blank_lines = filter(filter_blank_lines, no_comments)\n",
    "        no_trailing_whitespace = map(lambda s: s.strip(), no_blank_lines)\n",
    "        sections = separate_symbols_lexicon(no_trailing_whitespace)\n",
    "        \n",
    "        multichar_symbols = get_multichar_symbols(sections)\n",
    "        lexicons = get_lexicons(sections)\n",
    "        lexicons_rules = get_lexicons_rules(lexicons)\n",
    "        \n",
    "        return (multichar_symbols, lexicons_rules)\n",
    "\n",
    "(multichar_symbols, lexicons_rules) = read_lexc('apertium-tur.tur.lexc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('', '', 'Root'), ('', '', 'Verbs'), ('tümle', 'tümle', 'V-IR-TV'), ('%<v%>%<tv%>', '', 'V-DERIV-IR'), ('', '', 'V-DERIV-IR-COMMON'), ('', '', 'V-FIN-IR'), ('', '', 'V-TENSE-COMMON'), ('', '', 'V-TENSE-MA'), ('', '', 'V-TENSE-MA-COMMON'), ('', '', 'V-IMP'), ('%<imp%>%<p2%>%<sg%>', '', 'V-IFI-COND-PERS-QST')]]\n"
     ]
    }
   ],
   "source": [
    "def trace_stem(stem):\n",
    "    trace = dict()\n",
    "    stem_forms = expand_root(stem)\n",
    "    print(stem_forms)\n",
    "    for lform in list(stem_forms):\n",
    "        rforms = list(map(lambda x: x[0].replace('@_EPSILON_SYMBOL_@', ''), stem_forms[lform]))\n",
    "        lform = lform.replace('@_EPSILON_SYMBOL_@', '')\n",
    "        trace[lform] = rforms\n",
    "    return trace\n",
    "\n",
    "def trace_analysis(lexicons_rules, root, left, right):\n",
    "    def helper(path, left_current, right_current):\n",
    "        paths = []\n",
    "        rules = lexicons_rules[path[-1][2]]\n",
    "        for rule in rules:\n",
    "            if len(rule) == 1:\n",
    "                if rule[-1] == '#':\n",
    "                    if left_current == left and right_current == right:\n",
    "                        paths.append(path)\n",
    "                else:\n",
    "                    paths.extend(\n",
    "                        helper(path + [('', '', rule[-1])], \n",
    "                               left_current, \n",
    "                               right_current))\n",
    "            elif len(rule) == 2:\n",
    "                replacements = parse_replacement(rule[0])\n",
    "                \n",
    "                # TODO hacky workaround, replace\n",
    "                if len(replacements) != 2:\n",
    "                    continue\n",
    "                    \n",
    "                (left_new, right_new) = replacements\n",
    "                left_combined = left_current + left_new\n",
    "                right_combined = right_current + right_new\n",
    "                \n",
    "                if rule[-1] == '#':\n",
    "                    if left_combined == left and right_combined == right:\n",
    "                        paths.append(left_new, right_new, path)\n",
    "                elif left.startswith(left_combined) and right.startswith(right_combined):\n",
    "                    paths.extend(\n",
    "                        helper(\n",
    "                            path + [(left_new, right_new, rule[-1])], \n",
    "                            left_combined, \n",
    "                            right_combined))\n",
    "        return paths\n",
    "    return helper([('', '', root)], '', '')\n",
    "\n",
    "(multichar_symbols, lexicons_rules) = read_lexc('apertium-tur.tur.lexc')\n",
    "trace = trace_analysis(lexicons_rules, 'Root', 'tümle%<v%>%<tv%>%<imp%>%<p2%>%<sg%>', 'tümle')\n",
    "print(trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting compiling 0\n",
      "Starting intersection 11\n",
      "Starting extracting paths 119\n",
      "{'tümle<barb>': [('tümle@_EPSILON_SYMBOL_@', 0.0)], 'tümle<v><tv><imp><p2><sg>': [('tümle@_EPSILON_SYMBOL_@@_EPSILON_SYMBOL_@@_EPSILON_SYMBOL_@@_EPSILON_SYMBOL_@@_EPSILON_SYMBOL_@', 0.0)]}\n"
     ]
    }
   ],
   "source": [
    "import calendar, time\n",
    "start = calendar.timegm(time.gmtime())\n",
    "\n",
    "def expand_root(tr, root):\n",
    "    hfst_regex = '[ %s ][[ %s ][ ?* ]]:[ ?* ]' % (' '.join(root), ' | '.join(\n",
    "        map(\n",
    "            lambda s: \n",
    "                s.replace('<', '%<').replace('>', '%>').replace('_', '%_'), \n",
    "            get_tags())))\n",
    "    limiting_tr = hfst.regex(hfst_regex)\n",
    "    tr_copy = hfst.HfstTransducer(tr)\n",
    "    print('Starting intersection', (calendar.timegm(time.gmtime()) - start))\n",
    "    tr_copy.intersect(limiting_tr)\n",
    "    print('Starting extracting paths', (calendar.timegm(time.gmtime()) - start))\n",
    "    return tr_copy.extract_paths(max_cycles=16, max_number=256, output='dict')\n",
    "\n",
    "print('Starting compiling', (calendar.timegm(time.gmtime()) - start))\n",
    "tr = hfst.compile_lexc_file('apertium-tur.tur.lexc')\n",
    "tr.determinize()\n",
    "expanded = expand_root(tr, 'tümle')\n",
    "print(expanded)\n",
    "# print(expand_root('o'))\n",
    "\n",
    "# tr.determinize()\n",
    "# tr.invert()\n",
    "# print(tr.lookup(''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz\n",
    "\n",
    "def trim_unconnected_rules(lexicons_rules, root):\n",
    "    marked_tags = set()\n",
    "    def trim_helper(current_tag):\n",
    "        marked_tags.add(current_tag)\n",
    "        rules = lexicons_rules[current_tag]\n",
    "        for rule in rules:\n",
    "            if rule[-1] not in marked_tags and rule[-1] != '#':\n",
    "                trim_helper(rule[-1])\n",
    "    trim_helper(root)\n",
    "    \n",
    "    new_lexicons_rules = dict(lexicons_rules)\n",
    "    for tag in set(list(lexicons_rules)).difference(marked_tags):\n",
    "        del new_lexicons_rules[tag]\n",
    "        \n",
    "    return new_lexicons_rules\n",
    "\n",
    "def create_lexicon_dep_graph(lexicons_rules, output_filename):\n",
    "    dot = graphviz.Digraph(format='pdf')\n",
    "    existing_edges = dict()\n",
    "    \n",
    "    for lexicon_tag in list(lexicons_rules):\n",
    "        dot.node(lexicon_tag, lexicon_tag)\n",
    "    \n",
    "    for lexicon_tag in list(lexicons_rules):\n",
    "        rules = lexicons_rules[lexicon_tag]\n",
    "        for rule in rules:\n",
    "            if rule[-1] != '#':\n",
    "                if not lexicon_tag in existing_edges:\n",
    "                    existing_edges[lexicon_tag] = []\n",
    "                if rule[-1] not in existing_edges[lexicon_tag]:\n",
    "                    dot.edge(lexicon_tag, rule[-1])\n",
    "                    existing_edges[lexicon_tag].append(rule[-1])\n",
    "                \n",
    "    dot.render(output_filename, view=False, cleanup=True)\n",
    "    \n",
    "def create_highlighted_lexicon_dep_graph(lexicons_rules, annotated_highlights, output_filename):\n",
    "    dot = graphviz.Digraph(format='pdf')\n",
    "    existing_edges = dict()\n",
    "    \n",
    "    highlights = list(map(lambda x: list(map(lambda y: y[2], x)), annotated_highlights))\n",
    "    \n",
    "    dot.attr('node', shape='box')\n",
    "    for lexicon_tag in list(lexicons_rules):\n",
    "        suffix = ''\n",
    "        for highlight in annotated_highlights:\n",
    "            for l, r, tag in highlight:\n",
    "                if tag == lexicon_tag:\n",
    "                    dot.attr('node', color='red', style='filled')\n",
    "                    suffix = '\\n' + l + ':' + r if l != '' or r != '' else ''\n",
    "                    break\n",
    "        suffix = suffix.replace('%<', '<').replace('%>', '>')\n",
    "        dot.node(lexicon_tag, lexicon_tag + suffix)\n",
    "        dot.attr('node', color='black', fontcolor='black', style='')\n",
    "    \n",
    "    for lexicon_tag in list(lexicons_rules):\n",
    "        rules = lexicons_rules[lexicon_tag]\n",
    "        for rule in rules:\n",
    "            if rule[-1] != '#':\n",
    "                if not lexicon_tag in existing_edges:\n",
    "                    existing_edges[lexicon_tag] = []\n",
    "                if rule[-1] not in existing_edges[lexicon_tag]:\n",
    "                    for highlight in highlights:\n",
    "                        if lexicon_tag in highlight and rule[-1] in highlight:\n",
    "                            dot.attr('edge', color='red', arrowsize='2.0')\n",
    "                    dot.edge(lexicon_tag, rule[-1])\n",
    "                    existing_edges[lexicon_tag].append(rule[-1])\n",
    "                    dot.attr('edge', color='black', arrowsize='1.0')\n",
    "                \n",
    "    dot.render(output_filename, view=False, cleanup=True)\n",
    "    \n",
    "# create_lexicon_dep_graph(lexicons_rules, 'tur/Root')\n",
    "# for tag, in list(lexicons_rules['Root']):    \n",
    "#     trimmed = trim_unconnected_rules(lexicons_rules, tag)\n",
    "#     create_lexicon_dep_graph(trimmed, 'tur/' + tag)\n",
    "\n",
    "highlights = [[('', '', 'Root'), ('', '', 'Verbs'), ('tümle', 'tümle', 'V-IR-TV'), ('%<v%>%<tv%>', '', 'V-DERIV-IR'), ('', '', 'V-DERIV-IR-COMMON'), ('', '', 'V-FIN-IR'), ('', '', 'V-TENSE-COMMON'), ('', '', 'V-TENSE-MA'), ('', '', 'V-TENSE-MA-COMMON'), ('', '', 'V-IMP'), ('%<imp%>%<p2%>%<sg%>', '', 'V-IFI-COND-PERS-QST')]]\n",
    "trimmed = trim_unconnected_rules(lexicons_rules, 'Verbs')\n",
    "create_highlighted_lexicon_dep_graph(trimmed, highlights, 'tur-hl/tümle')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dapertium",
   "language": "python",
   "name": "dapertium"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
