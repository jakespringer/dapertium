{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'h.e.l.l.o'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'.'.join('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t1\tfoo\tbaz\t0\n",
      "1\t0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import hfst\n",
    "tr1 = hfst.regex('foo:bar')\n",
    "tr2 = hfst.regex('bar:baz')\n",
    "tr1.compose(tr2)\n",
    "print(tr1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== a ===\n",
      "=== b ===\n",
      "0\t1\tA\tA\t0\n",
      "1\t2\tb\tb\t0\n",
      "2\t3\ti\ti\t0\n",
      "3\t4\th\th\t0\n",
      "4\t5\tu\tu\t0\n",
      "5\t6\tt\tt\t0\n",
      "6\t7\t<n>\t@0@\t0\n",
      "6\t8\t<n>\tb\t0\n",
      "6\t9\t<n>\tn\t0\n",
      "6\t10\t<n>\ty\t0\n",
      "6\t11\t<n>\t{e}\t0\n",
      "6\t12\t<n>\tm\t0\n",
      "6\t13\t<n>\te\t0\n",
      "6\t14\t<n>\tf\t0\n",
      "6\t15\t<n>\t{a}\t0\n",
      "6\t16\t<n>\t{W}\t0\n",
      "6\t17\t<n>\ts\t0\n",
      "7\t0\n",
      "8\t18\t<relational>\ta\t0\n",
      "9\t19\t<subj>\ta\t0\n",
      "10\t20\t+\ta\t0\n",
      "11\t19\t<ins>\tm\t0\n",
      "12\t21\t<pos>\ta\t0\n",
      "13\t19\t<pos>\tm\t0\n",
      "14\t7\t<dat>\t@0@\t0\n",
      "14\t22\t<com>\ta\t0\n",
      "15\t23\t+\tp\t0\n",
      "16\t24\t+\te\t0\n",
      "17\t7\t<acc>\t@0@\t0\n",
      "18\t25\t@0@\tr\t0\n",
      "19\t0\n",
      "20\t26\ty\tn\t0\n",
      "21\t25\t@0@\tn\t0\n",
      "22\t27\t@0@\tr\t0\n",
      "23\t28\ta\t@0@\t0\n",
      "24\t29\tw\tr\t0\n",
      "25\t27\t@0@\ta\t0\n",
      "26\t30\ta\t@0@\t0\n",
      "27\t0\n",
      "28\t31\tp\t@0@\t0\n",
      "29\t32\te\t@0@\t0\n",
      "30\t33\tn\t@0@\t0\n",
      "31\t7\t<post>\t@0@\t0\n",
      "32\t31\tr\t@0@\t0\n",
      "33\t7\t<neg>\t@0@\t0\n",
      "\n",
      "Abihut<n>:Abihut@_EPSILON_SYMBOL_@\t0\n",
      "Abihut<n><relational>@_EPSILON_SYMBOL_@@_EPSILON_SYMBOL_@:Abihutbara\t0\n",
      "Abihut<n><pos>:Abihutem\t0\n",
      "Abihut<n><com>@_EPSILON_SYMBOL_@:Abihutfar\t0\n",
      "Abihut<n><dat>:Abihutf@_EPSILON_SYMBOL_@\t0\n",
      "Abihut<n><pos>@_EPSILON_SYMBOL_@@_EPSILON_SYMBOL_@:Abihutmana\t0\n",
      "Abihut<n><subj>:Abihutna\t0\n",
      "Abihut<n><acc>:Abihuts@_EPSILON_SYMBOL_@\t0\n",
      "Abihut<n>+yan<neg>:Abihutyan@_EPSILON_SYMBOL_@@_EPSILON_SYMBOL_@@_EPSILON_SYMBOL_@\t0\n",
      "Abihut<n>+wer<post>:Abihut{W}er@_EPSILON_SYMBOL_@@_EPSILON_SYMBOL_@@_EPSILON_SYMBOL_@\t0\n",
      "Abihut<n>+ap<post>:Abihut{a}p@_EPSILON_SYMBOL_@@_EPSILON_SYMBOL_@@_EPSILON_SYMBOL_@\t0\n",
      "Abihut<n><ins>:Abihut{e}m\t0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import hfst\n",
    "import sys\n",
    "\n",
    "a = hfst.compile_lexc_file('apertium-bkl.bkl.lexc')\n",
    "a.determinize()\n",
    "b = hfst.regex('[A b i h u t][?*]:[?*]')\n",
    "b.intersect(a)\n",
    "print('=== a ===')\n",
    "#print(a)\n",
    "print('=== b ===')\n",
    "print(b)\n",
    "print(b.extract_paths(max_cycles=16, max_number=200, output='text', random='True'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== a ===\n",
      "0\t1\thello\ta\t0\n",
      "0\t1\thi\ta\t0\n",
      "0\t1\tokay\ta\t0\n",
      "0\t2\tme\tme\t0\n",
      "0\t2\thello\thello\t0\n",
      "1\t1\t@0@\ta\t0\n",
      "1\t0\n",
      "2\t3\tthere\tb\t0\n",
      "3\t3\t@0@\tb\t0\n",
      "3\t0\n",
      "\n",
      "hello:a\t0\n",
      "hello@_EPSILON_SYMBOL_@:aa\t0\n",
      "hello@_EPSILON_SYMBOL_@@_EPSILON_SYMBOL_@:aaa\t0\n",
      "hello@_EPSILON_SYMBOL_@@_EPSILON_SYMBOL_@@_EPSILON_SYMBOL_@:aaaa\t0\n",
      "hello@_EPSILON_SYMBOL_@@_EPSILON_SYMBOL_@@_EPSILON_SYMBOL_@@_EPSILON_SYMBOL_@:aaaaa\t0\n",
      "hello@_EPSILON_SYMBOL_@@_EPSILON_SYMBOL_@@_EPSILON_SYMBOL_@@_EPSILON_SYMBOL_@@_EPSILON_SYMBOL_@:aaaaaa\t0\n",
      "hi:a\t0\n",
      "hi@_EPSILON_SYMBOL_@:aa\t0\n",
      "hi@_EPSILON_SYMBOL_@@_EPSILON_SYMBOL_@:aaa\t0\n",
      "hi@_EPSILON_SYMBOL_@@_EPSILON_SYMBOL_@@_EPSILON_SYMBOL_@:aaaa\t0\n",
      "\n",
      "=== b ===\n",
      "0\t1\thello\ta\t0\n",
      "0\t2\thello\thello\t0\n",
      "1\t3\t@_UNKNOWN_SYMBOL_@\t@0@\t0\n",
      "1\t3\tthere\t@0@\t0\n",
      "1\t3\thello\t@0@\t0\n",
      "1\t3\tokay\t@0@\t0\n",
      "1\t3\tme\t@0@\t0\n",
      "1\t3\thi\t@0@\t0\n",
      "1\t3\tb\t@0@\t0\n",
      "1\t3\ta\t@0@\t0\n",
      "1\t4\t@0@\ta\t0\n",
      "1\t0\n",
      "2\t5\t@_UNKNOWN_SYMBOL_@\tb\t0\n",
      "2\t5\ta\tb\t0\n",
      "2\t5\tb\tb\t0\n",
      "2\t5\thi\tb\t0\n",
      "2\t5\tme\tb\t0\n",
      "2\t5\tokay\tb\t0\n",
      "2\t5\tthere\tb\t0\n",
      "2\t5\thello\tb\t0\n",
      "2\t6\t@0@\tb\t0\n",
      "3\t3\t@_UNKNOWN_SYMBOL_@\t@0@\t0\n",
      "3\t3\thello\t@0@\t0\n",
      "3\t3\ta\t@0@\t0\n",
      "3\t3\tb\t@0@\t0\n",
      "3\t3\thi\t@0@\t0\n",
      "3\t3\tme\t@0@\t0\n",
      "3\t3\tokay\t@0@\t0\n",
      "3\t3\tthere\t@0@\t0\n",
      "3\t7\t@0@\ta\t0\n",
      "3\t0\n",
      "4\t4\t@0@\ta\t0\n",
      "4\t0\n",
      "5\t8\t@_UNKNOWN_SYMBOL_@\t@0@\t0\n",
      "5\t8\tthere\t@0@\t0\n",
      "5\t8\thello\t@0@\t0\n",
      "5\t8\tokay\t@0@\t0\n",
      "5\t8\tme\t@0@\t0\n",
      "5\t8\thi\t@0@\t0\n",
      "5\t8\tb\t@0@\t0\n",
      "5\t8\ta\t@0@\t0\n",
      "5\t9\t@0@\tb\t0\n",
      "5\t0\n",
      "6\t6\t@0@\tb\t0\n",
      "6\t0\n",
      "7\t7\t@0@\ta\t0\n",
      "7\t0\n",
      "8\t8\t@_UNKNOWN_SYMBOL_@\t@0@\t0\n",
      "8\t8\thello\t@0@\t0\n",
      "8\t8\ta\t@0@\t0\n",
      "8\t8\tb\t@0@\t0\n",
      "8\t8\thi\t@0@\t0\n",
      "8\t8\tme\t@0@\t0\n",
      "8\t8\tokay\t@0@\t0\n",
      "8\t8\tthere\t@0@\t0\n",
      "8\t10\t@0@\tb\t0\n",
      "8\t0\n",
      "9\t9\t@0@\tb\t0\n",
      "9\t0\n",
      "10\t10\t@0@\tb\t0\n",
      "10\t0\n",
      "\n",
      "hello:a\t0\n",
      "hello@_UNKNOWN_SYMBOL_@:a@_EPSILON_SYMBOL_@\t0\n",
      "hello@_UNKNOWN_SYMBOL_@@_EPSILON_SYMBOL_@:a@_EPSILON_SYMBOL_@a\t0\n",
      "hello@_UNKNOWN_SYMBOL_@@_EPSILON_SYMBOL_@@_EPSILON_SYMBOL_@:a@_EPSILON_SYMBOL_@aa\t0\n",
      "hello@_UNKNOWN_SYMBOL_@@_UNKNOWN_SYMBOL_@:a@_EPSILON_SYMBOL_@@_EPSILON_SYMBOL_@\t0\n",
      "hello@_UNKNOWN_SYMBOL_@@_UNKNOWN_SYMBOL_@@_EPSILON_SYMBOL_@:a@_EPSILON_SYMBOL_@@_EPSILON_SYMBOL_@a\t0\n",
      "hello@_UNKNOWN_SYMBOL_@@_UNKNOWN_SYMBOL_@@_EPSILON_SYMBOL_@@_EPSILON_SYMBOL_@:a@_EPSILON_SYMBOL_@@_EPSILON_SYMBOL_@aa\t0\n",
      "hello@_UNKNOWN_SYMBOL_@hello:a@_EPSILON_SYMBOL_@@_EPSILON_SYMBOL_@\t0\n",
      "hello@_UNKNOWN_SYMBOL_@hello@_EPSILON_SYMBOL_@:a@_EPSILON_SYMBOL_@@_EPSILON_SYMBOL_@a\t0\n",
      "hello@_UNKNOWN_SYMBOL_@hello@_EPSILON_SYMBOL_@@_EPSILON_SYMBOL_@:a@_EPSILON_SYMBOL_@@_EPSILON_SYMBOL_@aa\t0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import hfst\n",
    "\n",
    "a2 = hfst.regex('[ hello | me ] [ there ] :[b+]')\n",
    "a  = hfst.regex('[ hello | hi | okay ] :[a+]')\n",
    "a.priority_union(a2)\n",
    "a.determinize()\n",
    "\n",
    "\n",
    "a_basic = hfst.HfstBasicTransducer(a)\n",
    "b = hfst.regex('[ hello ][?*]:[?*]')\n",
    "ab = b.compose(a)\n",
    "\n",
    "print('=== a ===')\n",
    "print(a)\n",
    "# for arc in a_basic.transitions(0):\n",
    "#     print(arc)\n",
    "print(a.extract_paths(max_number=10, max_cycles=5, output='text'))\n",
    "print('=== b ===')\n",
    "print(b)\n",
    "print(b.extract_paths(max_number=10, max_cycles=1, output='text'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ga<conj>:ga@_EPSILON_SYMBOL_@\t0\n",
      "ga<part>:ga@_EPSILON_SYMBOL_@\t0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import hfst\n",
    "\n",
    "tr = hfst.compile_lexc_file('apertium-bkl.bkl.lexc')\n",
    "tr.determinize()\n",
    "\n",
    "def get_tags():\n",
    "    return list(filter(lambda s: s.startswith('<') and s.endswith('>'), tr.get_alphabet()))\n",
    "\n",
    "def expand_root(root):\n",
    "    hfst_regex = '[ %s ][[ %s ][ ?* ]]:[?*]' % (' '.join(root), ' | '.join(\n",
    "        map(\n",
    "            lambda s: s.replace('<', '%<').replace('>', '%>'), \n",
    "            get_tags())))\n",
    "    limiting_tr = hfst.regex(hfst_regex)\n",
    "    tr_copy = hfst.HfstTransducer(tr)\n",
    "    tr_copy.intersect(limiting_tr)\n",
    "    return tr_copy.extract_paths(max_cycles=16, max_number=256, output='text')\n",
    "    \n",
    "print(expand_root('ga'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import hfst\n",
    "\n",
    "class Lexc:\n",
    "    multichar_symbols = list()\n",
    "    lexicons = dict()\n",
    "    \n",
    "    def __init__(self, ms, ls):\n",
    "        self.multichar_symbols = ms\n",
    "        self.lexicons = ls\n",
    "\n",
    "def remove_comment_from_line(line):\n",
    "    return re.split('(?<!%)\\\\!', line)[0]\n",
    "\n",
    "def filter_blank_lines(line):\n",
    "    return line.strip() != ''\n",
    "\n",
    "def separate_symbols_lexicon(lines):\n",
    "    sections = []\n",
    "    current = []\n",
    "    for line in lines:\n",
    "        if line == 'Multichar_Symbols':\n",
    "            if len(current) > 0:\n",
    "                sections.append(current)\n",
    "            current = ['Multichar_Symbols']\n",
    "        elif line.startswith('LEXICON'):\n",
    "            if len(current) > 0:\n",
    "                sections.append(current)\n",
    "            current = [ line ]\n",
    "        else:\n",
    "            current.append(line)\n",
    "    return sections\n",
    "\n",
    "def get_multichar_symbols(sections):\n",
    "    for section in sections:\n",
    "        if len(section) > 0 and section[0] == 'Multichar_Symbols':\n",
    "            return section[1:]\n",
    "    return []\n",
    "\n",
    "def get_lexicons(sections):\n",
    "    lexicons = dict()\n",
    "    for section in sections:\n",
    "        if len(section) > 0 and section[0].startswith('LEXICON'):\n",
    "            x = len('LEXICON ')\n",
    "            name = section[0][x:]\n",
    "            lexicons[name] = list(map(lambda x: x.strip(), ''.join(section[1:]).split(';')))\n",
    "    return lexicons\n",
    "\n",
    "def tokenize_rule(rule):\n",
    "    return re.split('(?<!%)\\\\s', rule)\n",
    "\n",
    "def parse_rules(lexicon):\n",
    "    return list(filter(lambda x: x != [''], map(tokenize_rule, lexicon)))\n",
    "\n",
    "def parse_replacement(replacement):\n",
    "    return re.split('(?<!%)\\\\:', replacement)\n",
    "\n",
    "def get_lexicons_rules(lexicons):\n",
    "    lexicons_rules = dict()\n",
    "    for tag in list(lexicons):\n",
    "        lexicons_rules[tag] = parse_rules(lexicons[tag])\n",
    "    return lexicons_rules\n",
    "\n",
    "def identify_roots(lexicons_rules):\n",
    "    def find_first_replacements(lexicon_tag):\n",
    "        if lexicon_tag in lexicons_rules:\n",
    "            replacements = []\n",
    "            lexicon_rules = lexicons_rules[lexicon_tag]\n",
    "            for rule in lexicon_rules:\n",
    "                if len(rule) == 2:\n",
    "                    replacements.append(parse_replacement(rule[0]))\n",
    "                elif len(rule) == 1:\n",
    "                    replacements.extend(find_first_replacements(rule[0]))\n",
    "            return replacements\n",
    "        else:\n",
    "            return []\n",
    "        \n",
    "    return find_first_replacements('Root')\n",
    "            \n",
    "tr = hfst.compile_lexc_file('apertium-bkl.bkl.lexc')\n",
    "tr.determinize()\n",
    "\n",
    "def get_tags():\n",
    "    return list(filter(lambda s: s.startswith('<') and s.endswith('>'), tr.get_alphabet()))\n",
    "\n",
    "def expand_root(root):\n",
    "    hfst_regex = '[ %s ][[ %s ][ ?* ]]:[ ?* ]' % (' '.join(root), ' | '.join(\n",
    "        map(\n",
    "            lambda s: s.replace('<', '%<').replace('>', '%>'), \n",
    "            get_tags())))\n",
    "    limiting_tr = hfst.regex(hfst_regex)\n",
    "    tr_copy = hfst.HfstTransducer(tr)\n",
    "    tr_copy.intersect(limiting_tr)\n",
    "    return tr_copy.extract_paths(max_cycles=16, max_number=256, output='text')\n",
    "    \n",
    "def read_lexc(lexc_filename):\n",
    "    with open(lexc_filename) as lexc_file:\n",
    "        lexc = lexc_file.read()\n",
    "        lexc_lines = lexc.splitlines()\n",
    "        no_comments = map(remove_comment_from_line, lexc_lines)\n",
    "        no_blank_lines = filter(filter_blank_lines, no_comments)\n",
    "        no_trailing_whitespace = map(lambda s: s.strip(), no_blank_lines)\n",
    "        sections = separate_symbols_lexicon(no_trailing_whitespace)\n",
    "        \n",
    "        multichar_symbols = get_multichar_symbols(sections)\n",
    "        lexicons = get_lexicons(sections)\n",
    "        lexicons_rules = get_lexicons_rules(lexicons)\n",
    "        \n",
    "        return (multichar_symbols, lexicons_rules)\n",
    "    \n",
    "#         roots = identify_roots(lexicons_rules)\n",
    "        \n",
    "#         for (root, _) in roots:\n",
    "#             print('========', root, '========')\n",
    "#             print(expand_root(root))\n",
    "\n",
    "(multichar_symbols, lexicons_rules) = read_lexc('apertium-tur.tur.lexc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz\n",
    "\n",
    "def create_lexicon_dep_graph(lexicons_rules, output_filename):\n",
    "    dot = graphviz.Digraph()\n",
    "    existing_edges = dict()\n",
    "    \n",
    "    for lexicon_tag in list(lexicons_rules):\n",
    "        dot.node(lexicon_tag, lexicon_tag)\n",
    "    \n",
    "    for lexicon_tag in list(lexicons_rules):\n",
    "        rules = lexicons_rules[lexicon_tag]\n",
    "        for rule in rules:\n",
    "            if rule[-1] != '#':\n",
    "                if not lexicon_tag in existing_edges:\n",
    "                    existing_edges[lexicon_tag] = []\n",
    "                if rule[-1] not in existing_edges[lexicon_tag]:\n",
    "                    dot.edge(lexicon_tag, rule[-1])\n",
    "                    existing_edges[lexicon_tag].append(rule[-1])\n",
    "                \n",
    "    dot.render(output_filename, view=True)\n",
    "    \n",
    "create_lexicon_dep_graph(lexicons_rules, 'output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dapertium",
   "language": "python",
   "name": "dapertium"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
